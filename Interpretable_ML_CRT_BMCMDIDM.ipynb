{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "installPreReqs=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if installPreReqs:\n",
    "    !pip install xgboost\n",
    "    !pip install shap\n",
    "    !pip install pandas\n",
    "    !pip install numpy\n",
    "    !pip install scipy\n",
    "    !pip install matplotlib\n",
    "    !pip install scipy\n",
    "    !pip install seaborn\n",
    "    !pip install catboost\n",
    "    #!pip install skopt\n",
    "    !pip install mlens\n",
    "    !pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports - Needs cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import scipy as sp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,KFold\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,log_loss,f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,VotingClassifier,StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#import mlens\n",
    "#import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0:1\"\n",
    "pd.options.display.max_colwidth = 60\n",
    "#remove warnings for final output\n",
    "warnings.filterwarnings('ignore')\n",
    "seed=4242\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#enables model selection search\n",
    "modelSelectionSearch=True\n",
    "lowPriority=True\n",
    "modelType=CatBoostClassifier(silent=True,task_type=\"CPU\",eval_metric='F1', random_strength=.2, border_count=32,iterations=250,\n",
    "                             bagging_temperature=.03,max_ctr_complexity=1,l2_leaf_reg=100,depth=6,learning_rate=.03)\n",
    "#Degree of interaction features to create\n",
    "degreeInteractions=0\n",
    "#Try scaling features\n",
    "scaleX=False\n",
    "#Search stacking models\n",
    "tryStacking=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpriority():\n",
    "    \"\"\" Set the priority of the process to below-normal.\"\"\"\n",
    "\n",
    "    import sys\n",
    "    try:\n",
    "        sys.getwindowsversion()\n",
    "    except AttributeError:\n",
    "        isWindows = False\n",
    "    else:\n",
    "        isWindows = True\n",
    "\n",
    "    if isWindows:\n",
    "        # Based on:\n",
    "        #   \"Recipe 496767: Set Process Priority In Windows\" on ActiveState\n",
    "        #   http://code.activestate.com/recipes/496767/\n",
    "        import win32api,win32process,win32con\n",
    "\n",
    "        pid = win32api.GetCurrentProcessId()\n",
    "        handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\n",
    "        win32process.SetPriorityClass(handle, win32process.BELOW_NORMAL_PRIORITY_CLASS)\n",
    "    else:\n",
    "        import os\n",
    "\n",
    "        os.nice(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(lowPriority):\n",
    "    lowpriority()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(df) ## Use the CRT data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First visit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstVisit =df['time']==0\n",
    "thirdMonth =df['time']==3\n",
    "sixthMonth =df['time']==6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[firstVisit]\n",
    "df.set_index('Patno',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Did not expect the data types in fields EDV/ESV, Gender, LVEF, LVEF_DC, NYHA, NYHA_Description, PRIMARY, PRIMARYA, Patient_Status, Position_LAO_View, Position_RAO_View, Randomization_Group, , centersize, cube root, group,\n",
    "\n",
    "del df['LVEF']\n",
    "del df['LVESV6'] #result set\n",
    "del df['VT'] #use vt types\n",
    "del df['bb'] #duplicate\n",
    "del df['isch'] #duplicate\n",
    "del df['Sample']\n",
    "del df['EDV/ESV']\n",
    "del df['normal_indicator'] #static value\n",
    "del df['pstatus']\n",
    "del df['Patient_Status'] #static\n",
    "del df['Visit'] #same as time\n",
    "del df['statuschange'] #empty\n",
    "del df['Randomization_Group']\n",
    "del df['resp'] #duplicate to response\n",
    "del df['NYHA_Description']\n",
    "del df['cube root']\n",
    "del df['substudy']\n",
    "del df['death']\n",
    "del df['PRIMARY'] #manually divided\n",
    "del df['PRIMARYA']\n",
    "#del df['group']\n",
    "del df['Position_RAO_View']\n",
    "del df['Position_LAO_View']\n",
    "del df['time'] #not needed\n",
    "del df['lbbb'] #duplicate of history_of_left\n",
    "del df['LVEF_DC'] #noise\n",
    "del df['base_LVESV'] #remove from course 1\n",
    "del df['qrs'] #i think this is an after measureement\n",
    "del df['AV_INTERVAL_WITH_ATRIAL_PACING__'] #only available after crt\n",
    "df['EDV_ESV'] = df.LVEDV/ df.LVESV #testing\n",
    "df['LVEF'] = 100*(df.LVEDV - df.LVESV)/df.LVEDV\n",
    "df['cube_root']=df.EDV_ESV**(1/3)\n",
    "\n",
    "df['tachycardia']=df.apply(lambda x: 1 if x['hrrest']>100 else 0, axis=1)\n",
    "df['bradycardia']=df.apply(lambda x: 1 if x['hrrest']<60 else 0, axis=1)\n",
    "\n",
    "#assuming crp in ug not ng as paper suggests (due to ranges)\n",
    "#setting normal cutoff at 10000ug based on\n",
    "#https://www.mayoclinic.org/tests-procedures/c-reactive-protein-test/about/pac-20385228#:~:text=For%20a%20standard%20CRP%20test,testing%20to%20determine%20the%20cause.\n",
    "\n",
    "df['high_crp']=df.apply(lambda x: 1 if x['crp']>10000 else 0, axis=1)\n",
    "\n",
    "\n",
    "df['normalbp']=df.apply(lambda x: 1 if x['bpsys']<120 and x['bpdia']<80  else 0, axis=1)\n",
    "df['s1hypertension']=df.apply(lambda x: 1 if (x['bpsys']>=130 and x['bpsys']<=139) or (x['bpdia']>=80 and x['bpdia']<=89)  else 0, axis=1)\n",
    "df['s2hypertension']=df.apply(lambda x: 1 if x['bpsys']>=140 or x['bpdia']>=90  else 0, axis=1)\n",
    "##using classes from \n",
    "#https://www.cdc.gov/obesity/adult/defining.html#:~:text=If%20your%20BMI%20is%2018.5,falls%20within%20the%20obese%20range.\n",
    "\n",
    "df['obesity_c1']=df.apply(lambda x: 1 if x['bmi']>=30 and x['bmi']<35  else 0, axis=1)\n",
    "df['obesity_c2']=df.apply(lambda x: 1 if x['bmi']>=35 and x['bpdia']<40  else 0, axis=1)\n",
    "df['obesity_c3']=df.apply(lambda x: 1 if x['bmi']>=40 else 0, axis=1)\n",
    "df['underweight']=df.apply(lambda x: 1 if x['bmi']<18.5 else 0, axis=1)\n",
    "df['normalweight']=df.apply(lambda x: 1 if x['bmi']>=18.5 and x['bmi']<25 else 0, axis=1)\n",
    "#df['crt_response']=df.apply(lambda x: 1 if abs(x['lvesv_delta'])>15 else 0, axis=1)\n",
    "#normal qrs width = 120 > = wide\n",
    "#should this be 145? \n",
    "#https://watermark.silverchair.com/eup258.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAq0wggKpBgkqhkiG9w0BBwagggKaMIIClgIBADCCAo8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRWMvb5zxqoL3gsiSAgEQgIICYEBKxKn7lMiuicw1bf-sK96doZtlZfO_UITAQA9WmknUC3ab8yEoY_RSOfYP0BeWNCjXWabG9U_CQk05x_-HbOXozkMWvXuYDi2VvrF5XQRAyrSa3EAGp9DRtmgVt2btKil-ACIPBCtNL-saaq-6EGdo5EPB7f-NdTlqWvwbdngOdTUax-hTagz4jB1OYE0CPjMAcmafbEFjvNagVYrQB4j6BEo434Skq33YXNXgaqfCW5a1NdqQC07MXU1P3ASq_xL-BbJSGgv_67dh0SxIVPqftjShBTz6OYbcUtp1PrVClsehK4jTlGHYrqoJn-fnZOC_8ZnEaTEUlKgepco_sYvhiiGGkPDTwI2YwRC0o3j7MQzBn3di2bVWs1xThW2PyA9L6Eb4cpOe2LZHXnCp12P_3nZSzmsSMoOHhXfuy7CV_6m7IEY0cumjua2bqlpiIwpGNXtUWllBIaol5897lFJIq6NFJsqeBSHg0FOoSDvd7HhBHb7JHkhuZ_uQxcL-YJpZUBQ17bwlZcXtqsWlBZp00IR3o-mcteIR8gMDMGwuQErbXcyd3erE8AfM-B-UOir0NJQqtAuO_SvXiuePg6JKVxbOtSyobjQ1rqPQ5cbsxtm-sFXDHJEallvJYcdPsHxPueBsvTQ9pR1jOTfq-UFMRSNVSwmNT2yuFwXlTUkVmYFBoTi7X-WuJaBoH2jvo_U2AQADeYz4hk-bHtthaXIqAo6k1Vc61uhhJyr_7Xt5pdZcpNDZJblO8VFzsvW4l9-7FM2xs3MWAzF2NYatB7bKdCZURbXF-PBa3ZvdKZqr\n",
    "#df['normalQRS']=df.apply(lambda x: 1 if x['QRS_WIDTH_WITHOUT_ATRIAL_PACING']>70 and x['QRS_WIDTH_WITHOUT_ATRIAL_PACING']<110 else 0, axis=1)\n",
    "df['wideQRSwidth']=df.apply(lambda x: 1 if x['QRS_WIDTH_WITHOUT_ATRIAL_PACING']>120 else 0, axis=1)\n",
    "df['optimalAVnoPacing']=df.apply(lambda x: 1 if x['AV_INTERVAL_WITHOUT_ATRIAL_PACIN']>=100 and  x['AV_INTERVAL_WITHOUT_ATRIAL_PACIN']<=150 else 0, axis=1)\n",
    "#feature removal testing\n",
    "del df['hrrest'] #use tachy/brady feature\n",
    "#del df['height2'] #reduce multicoll.\n",
    "#del df['weight2'] #reduce multicoll.\n",
    "del df['MR_dP_dt__mm_Hg_s_1_'] #missing data too large\n",
    "del df['MR_Area__cm2_'] #missing data too large\n",
    "del df['MR_Width__mm_'] #missing data too large\n",
    "\n",
    "#del df['QRS_WIDTH_WITHOUT_ATRIAL_PACING'] #use wide measurement\n",
    "#del df['AV_INTERVAL_WITHOUT_ATRIAL_PACIN'] #use optimal feature\n",
    "\n",
    "##experiment\n",
    "#del df['bmi'] #multicol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperColumn(df, col):\n",
    "    df[col] = df[col].str.upper()\n",
    "    return df\n",
    "\n",
    "def oneHot(df, col, prefix):\n",
    "    df=upperColumn(df,col)\n",
    "    Oneht = pd.get_dummies(df[col], prefix=prefix, drop_first=True)\n",
    "    df = pd.concat([df, Oneht], axis=1)\n",
    "    del df[col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=oneHot(df,'group','grp_')\n",
    "#df=oneHot(df,'Position_RAO_View','PosRAO_')\n",
    "#df=oneHot(df,'Position_LAO_View','PosLAO_')\n",
    "df=oneHot(df,'NYHA','NYHA_')\n",
    "#df=oneHot(df,'LVEF_DC','LVEF_DC_')\n",
    "df=oneHot(df,'Gender','Gender_')\n",
    "df=oneHot(df,'centersize','centersize_')\n",
    "df=oneHot(df,'AT Type','AT_')\n",
    "df=oneHot(df,'VT Type','VT_')\n",
    "#cleanup columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get CRT score based on article methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCRTScore(row):\n",
    "    crtScore=0\n",
    "    if row['tnfrii']>=7090 :\n",
    "        crtScore+=1\n",
    "    if row['st2']>=23721 :\n",
    "        crtScore+=1\n",
    "    if row['crp']>=7381:\n",
    "        crtScore+=1\n",
    "    if row['mmp2']>=982000 :\n",
    "        crtScore+=1\n",
    "    return crtScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crt_score']=df.apply(lambda row: getCRTScore(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.68 pruning at .25 pearsonR\n",
    "#drop column\n",
    "###AV INTERVAL dropped after imputation\n",
    "#df.drop('AV_INTERVAL_WITHOUT_ATRIAL_PACIN', axis=1, inplace=True)\n",
    "#df.drop('EDV/ESV', axis=1, inplace=True)\n",
    "#df.drop('cube root', axis=1, inplace=True)\n",
    "#df.drop('bmi', axis=1, inplace=True)\n",
    "#absLVESV/LVEDV\n",
    "#df.drop('LVEDV', axis=1, inplace=True) #hurt model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Impute using XGB (Unused)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#this imputation did not improve model, but attempts to find using xgboost prediction\n",
    "#XGB picked arbitararily, if this is to be used should be tested/selected then tuned.\n",
    "def predictMissingCon(df,missingColumn):\n",
    "    #Impute using prediction\n",
    "    #def predictMissingValueMinValue(df):\n",
    "    #Predict __Minute_Value\n",
    "\n",
    "    #isolate missing feature\n",
    "    missingValueDF=df[missingColumn].isnull()\n",
    "    missingValue=df[df[missingColumn].isnull()]\n",
    "    df2=df.dropna()\n",
    "\n",
    "    X_pred=missingValue.loc[:, df.columns.difference([missingColumn, 'Patno'])]\n",
    "\n",
    "    y1 = df2[missingColumn]\n",
    "    X1=df2.loc[:, df.columns.difference([missingColumn, 'Patno'])]\n",
    "\n",
    "    clf = XGBClassifier()\n",
    "    clf.fit(X1, y1.values.ravel())\n",
    "    #missingPred=pd.Series(clf.predict(X_pred))\n",
    "    missingPred=clf.predict(X_pred)\n",
    "    #merge predictions in original df\n",
    "    missingValue[missingColumn]=pd.Series(clf.predict(X_pred), index=X_pred.index)\n",
    "    df=df.dropna(subset=[missingColumn], inplace=False)\n",
    "    df=df.append(missingValue)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Perform Stratified KFold (5) on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use stratified kfold cross validation of 5 splits\n",
    "\n",
    "def cvModel(model,label):\n",
    "    print('5-fold cross validation:\\n')\n",
    "    #kf=KFold(n_splits=5, random_state=1)\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    scores = cross_val_score(model, X_train,y_train, cv=kf, scoring ='f1')\n",
    "    recall_score=cross_val_score(model, X_train,y_train, cv=kf, scoring ='recall')\n",
    "    print(\" f1: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    return {'avg':scores.mean(), 'std': scores.std()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute or drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This did not improve the model, but more time should be used to tune the approach.\n",
    "#Attempt to predict missing values w/ regression & categorical w/ most frequent\n",
    "\n",
    "#df=predictMissingCon(df,'timp4')\n",
    "#df=predictMissingCon(df,'sgp130')\n",
    "#df=predictMissingCon(df,'sil2ra')\n",
    "#df=predictMissingCon(df,'hrrest')\n",
    "#df=predictMissingCon(df,'qol_score')\n",
    "#df=predictMissingCon(df,'AV_INTERVAL_WITHOUT_ATRIAL_PACIN')\n",
    "#df=predictMissingCon(df,'QRS_WIDTH_WITHOUT_ATRIAL_PACING')\n",
    "#df=predictMissingCon(df,'PR_INTERVAL_WITHOUT_ATRIAL_PACIN')\n",
    "#df=predictMissingCon(df,'__Minute_Walk')\n",
    "#df=predictMissingCon(df,'QRS_WIDTH_WITHOUT_ATRIAL_PACING') #makes model worse\n",
    "#categorical\n",
    "#impute categorical values with most frequent\n",
    "print(\"Missing Records before imputation\")\n",
    "print(df.isnull().sum())\n",
    "imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "n = [\"cabg\",\"pci\"]\n",
    "for n in n:\n",
    "    imputer2 = imputer2.fit(df[[n]])\n",
    "    df[n] = imputer2.transform(df[[n]])\n",
    "print(\"Dropping remaining missing values\")\n",
    "#drop AV aftger predicting PR\n",
    "#df.drop('AV_INTERVAL_WITHOUT_ATRIAL_PACIN', axis=1, inplace=True)\n",
    "print(\"Missing Records after imputation\")\n",
    "print(df.isnull().sum())\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Set labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = df['response']\n",
    "#remove unwanted columns\n",
    "X = df.loc[:, df.columns.difference(['response', 'Patno','2 clusters','3 clusters','4 clusters','lvesv_delta','change','LVESV6','death'])]\n",
    "#X = df.loc[:, df.columns.difference(['crt_response', 'Patno','2 clusters','3 clusters','4 clusters','lvesv_delta','LVEF','cube root','Pulse','LVEDV','bmi'])]\n",
    "feature_names=X.columns\n",
    "print(feature_names)\n",
    "y=pd.DataFrame(y)\n",
    "X = pd.DataFrame(X, columns =feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (degreeInteractions>0):\n",
    "    print(\"Adding \"+str(degreeInteractions)+\" degree interactions\")\n",
    "    X = PolynomialFeatures(degreeInteractions, interaction_only=True, include_bias=False).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#StandardScaler seems to perform best on data\n",
    "if scaleX :\n",
    "    #scaler = MinMaxScaler()\n",
    "    scaler=RobustScaler()\n",
    "    print(\"Scaling data\")\n",
    "    #scaler=StandardScaler()\n",
    "    X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Splitting Dataset for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset using a 80:20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from Anamul EDA notebook\n",
    "\n",
    "# Function to automate the fitting, evaluation and confusion matrix\n",
    "def classifier(model, feature_train, label_train, feature_test, label_test):\n",
    "    model.fit(feature_train,label_train)\n",
    "    label_predict = model.predict(feature_test)\n",
    "    print(classification_report(label_test,label_predict))\n",
    "    #Confusion matrix in the same way\n",
    "    cf_matrix = confusion_matrix(label_test, label_predict)\n",
    "    sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "    #print( cf_matrix)\n",
    "    accuracy=metrics.accuracy_score(label_test, label_predict)\n",
    "    precision= metrics.precision_score(label_test, label_predict)\n",
    "    recall=metrics.recall_score(label_test, label_predict)\n",
    "    roc_auc=metrics.roc_auc_score(label_test, label_predict)\n",
    "    f1=metrics.f1_score(label_test, label_predict, average='weighted')\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(label_test, label_predict))\n",
    "    #print(\"Precision:\",metrics.precision_score(label_test, label_predict))\n",
    "    #print(\"Recall:\",metrics.recall_score(label_test, label_predict))\n",
    "    #pl.show()\n",
    "    return (accuracy,precision,recall,f1,roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search classifiers for best AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#select best model based on weighted F1 Score of train cv\n",
    "\n",
    "def modelSearch(classifiers):\n",
    "   \n",
    "    estimatorResults=[]\n",
    "    best_model_f1=0\n",
    "    cvF1Avg=0\n",
    "    cvF1Std=0\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    display(Markdown(\"\\n **Searching for best model...**\\n\\n\"))\n",
    "    for clf in classifiers:\n",
    "            #find features\n",
    "            rfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(5),\n",
    "            scoring='roc_auc')\n",
    "            rfecv.fit(X_train, y_train.values.ravel())\n",
    "            #end find features\n",
    "            name = clf.__class__.__name__\n",
    "            display(Markdown(\"\\n **\"+name+\"**\"))\n",
    "            display(Markdown(\"\\n **Train Data Results**\\n\\n\"))\n",
    "            #cvScore=cvModel(clf,name)\n",
    "            scores=cross_val_score(clf, X_train, y_train, scoring='f1', cv=skf)\n",
    "            clf.fit(X_train, y_train.values.ravel())\n",
    "            pred=clf.predict(X_test)\n",
    "            model_f1=f1_score(y_test, pred, average='weighted')\n",
    "            if(model_f1>best_model_f1):\n",
    "                best_model_f1=model_f1\n",
    "                #best_clf=clf\n",
    "            #if(cvScore['avg']>cvF1Avg):\n",
    "            #    cvF1Avg=cvScore['avg']\n",
    "            #    best_clf=clf\n",
    "            #print(classification_report(y_test, pred))\n",
    "            display(Markdown(\"\\n **Test Data Results**\\n\\n\"))\n",
    "            testResult=classifier(clf,X_train,y_train,X_test,y_test)\n",
    "            testAcc=testResult[0]\n",
    "            testPrecision=testResult[1]\n",
    "            testRecall=testResult[2]\n",
    "            testF1=testResult[3]\n",
    "            testAUC=testResult[4]\n",
    "            estimatorResults.append((name,feature_names,clf,scaleX,degreeInteractions,scores,scores.mean(),scores.std(),testAcc,testPrecision,testRecall,testF1,testAUC))\n",
    "            print('TrainMean:'+str(scores.mean()))\n",
    "            print('TrainStd:'+str(scores.std()))\n",
    "    return estimatorResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for RandomForest Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid Search for GradientBoostingClassifier\n",
    "modelFileName=Path('models\\RF.pk1')\n",
    "\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    rfBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    #Grid Search for RF\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000],\n",
    "        'random_state':[42]\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    rf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    grid_search.best_params_\n",
    "    {'bootstrap': True,\n",
    "     'max_depth': 80,\n",
    "     'max_features': 3,\n",
    "     'min_samples_leaf': 5,\n",
    "     'min_samples_split': 12,\n",
    "     'n_estimators': 100}\n",
    "    rfBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(rfBestModel, modelFileName)\n",
    "print(rfBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "#Grid Search for SVC\n",
    "modelFileName=Path('models\\SVC.pk1')\n",
    "\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    svcBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'kernel': ['linear'],\n",
    "        'gamma': ['auto','scale'],\n",
    "        'probability': [True,False],\n",
    "        'C':[1,52,10],\n",
    "        'degree':[3,8],\n",
    "        'random_state':[42]\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    X_train_tmp=X_train.copy()\n",
    "    scaler=StandardScaler()\n",
    "    print(\"Scaling data\")\n",
    "    #scaler=StandardScaler()\n",
    "    X_train_tmp=scaler.fit_transform(X_train_tmp)\n",
    "    \n",
    "    grid_search = GridSearchCV(SVC(),\n",
    "                                 param_grid,\n",
    "                                 cv=skf,\n",
    "                                 verbose=5,\n",
    "                                 n_jobs = -1,\n",
    "                                scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train_tmp, y_train)\n",
    "    svcBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(svcBestModel, modelFileName)\n",
    "print(svcBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for KNN\n",
    "modelFileName=Path('models\\KNN.pk1')\n",
    "\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    knnBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    #Grid Search for RF\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'n_neighbors': [5,10,15,20,25],\n",
    "        'weights': ['uniform','distance'],\n",
    "        'algorithm': ['ball_tree','kd_tree','brute'],\n",
    "        'leaf_size': [15,30 ,60,90],\n",
    "        'p': [1,2],\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(estimator = knn, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    knnBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(knnBestModel, modelFileName)\n",
    "print(knnBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for GradientBoostingClassifier\n",
    "modelFileName=Path('models\\XGB.pk1')\n",
    "\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    xgBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    #Grid Search for XGB\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'n_estimators': [400, 700, 1000],\n",
    "        'learning_rate':[.25,.50,.1,.15,.2,.25,.30],\n",
    "        'colsample_bytree': [0.7, 0.8],\n",
    "        'max_depth': [3,4,5,6,7,8,9,10],\n",
    "        'reg_alpha': [1.1, 1.2, 1.3],\n",
    "        'reg_lambda': [1.1, 1.2, 1.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'eval_metric':['auc'],\n",
    "        'use_label_encoder':[False],\n",
    "        #'tree_method':['gpu_hist'],\n",
    "        #'gpu_id':[-1],\n",
    "        #'predictor':['gpu_predictor']\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    xg = XGBClassifier()\n",
    "    grid_search = GridSearchCV(estimator = xg, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc',)\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    xgBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(xgBestModel, modelFileName)\n",
    "print(xgBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for GradientBoostingClassifer Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid Search for GradientBoostingClassifier\n",
    "modelFileName=Path('models\\GBC.pk1')\n",
    "#remove not\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    gbcBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [3, 4, 5],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    gb = GradientBoostingClassifier()\n",
    "    grid_search = GridSearchCV(estimator = gb, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    grid_search.best_params_\n",
    "    {'max_depth': 80,\n",
    "     'max_features': 3,\n",
    "     'min_samples_leaf': 5,\n",
    "     'min_samples_split': 12,\n",
    "     'n_estimators': 100}\n",
    "    gbcBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(gbcBestModel, modelFileName)\n",
    "print(gbcBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for Adaboost\n",
    "#modelFileName=Path('models\\ada.pk1')\n",
    "modelFileName=Path('C:\\\\Users\\\\Doug\\\\explain\\\\models\\\\ada.pk1')\n",
    "#remove not\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    adaBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'n_estimators': [25,50,100, 200, 300, 1000],\n",
    "        'learning_rate':[.25,.50,.1,.15,.2,.25,.30]\n",
    "    }\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=5)\n",
    "    gb = AdaBoostClassifier()\n",
    "    grid_search = GridSearchCV(estimator = gb, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    adaBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(adaBestModel, modelFileName)\n",
    "print(adaBestModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for CatBoost Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid search for CB, parameters are for second tuning round.  Initial parameters were wider.\n",
    "modelFileName=Path('models\\CB.pk1')\n",
    "#remove not\n",
    "if modelFileName.is_file():\n",
    "    print(\"Loading model from disk...\")\n",
    "    cbBestModel=joblib.load(modelFileName)\n",
    "else:\n",
    "    param_grid = {'depth':[3,1,2,6,4,5,7,8,9,10],\n",
    "                  'iterations':[30,50,100,250,500],\n",
    "                  'learning_rate':[0.03,0.01,0.001],\n",
    "                  'l2_leaf_reg':[3,1,5,10,100],\n",
    "                  'random_strength':[0.2,0.5,0.8],\n",
    "                  'random_seed': [1],\n",
    "                  'logging_level':['Silent'],\n",
    "                  'od_type' : [\"Iter\"],\n",
    "                  'od_wait' : [100]\n",
    "                  }\n",
    "\n",
    "    cb = CatBoostClassifier(task_type=\"CPU\")\n",
    "    skf=StratifiedKFold(shuffle=True, n_splits=3)\n",
    "    grid_search = GridSearchCV(estimator = cb, param_grid = param_grid, \n",
    "                              cv = skf, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    cbBestModel = grid_search.best_estimator_\n",
    "    joblib.dump(cbBestModel, modelFileName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Classifers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#ModelName\tModel\tResults\t\n",
    "#estimatorResults=[]\n",
    "#Models to test\n",
    "\n",
    "classifiers = [\n",
    "        KNeighborsClassifier(5),\n",
    "        SVC(),\n",
    "        DecisionTreeClassifier(random_state=0),\n",
    "        #hyperparameters tuned for RF w/ grid search\n",
    "        rfBestModel,\n",
    "        AdaBoostClassifier(LogisticRegression()),\n",
    "        gbcBestModel,\n",
    "        GaussianNB(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        xgBestModel,\n",
    "        cbBestModel\n",
    "        ]\n",
    "\n",
    "\n",
    "#metaCLF=LogisticRegression()\n",
    "#Check if model search is necessary\n",
    "if(modelSelectionSearch):\n",
    "    estimatorResults=modelSearch(classifiers)\n",
    "else:\n",
    "    best_clf=modelType\n",
    "    #best_clf.fit(X_train, y_train.values.ravel(), plot=True)\n",
    "    \n",
    "#pred=best_clf.predict(X_test)\n",
    "#display model selection\n",
    "#best_model_f1=f1_score(y_test, pred, average='weighted')\n",
    "#select based on train score, but display test score\n",
    "#display(Markdown(\"\\n  Selected **\"+best_clf.__class__.__name__+\"** with test F1 score:**\"+str(round(best_model_f1,4))+\"**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "#estimator=[('lda',LinearDiscriminantAnalysis()),('cb',cbBestModel),('gbc',gbcBestModel),('rf',rfBestModel),\n",
    "#                   ('knn',KNeighborsClassifier(5))                   ,('ada',AdaBoostClassifier(LogisticRegression()))\n",
    "#                   ,('xgb',xgBestModel),('gnb',GaussianNB()),('svc',SVC())]\n",
    "#Train CV Mean Score: 0.7894 +/- 0.1264\n",
    "#test .70\n",
    "\n",
    "#train Train CV Mean Score: 0.7836 +/- 0.1011\n",
    "#test .6920\n",
    "\n",
    "#Train CV Mean Score: 0.7804 +/- 0.0122\n",
    "#Test ROC_AUC:0.7046677215189874\n",
    "xgbModel=xgBestModel\n",
    "    #knn\n",
    "knnModel= knnBestModel\n",
    "    #svc\n",
    "svcModel=svcBestModel\n",
    "    #rf\n",
    "rfModel=rfBestModel\n",
    "    #gbc\n",
    "gbcModel=gbcBestModel\n",
    "    #lda\n",
    "ldaModel= LinearDiscriminantAnalysis()\n",
    "    #mlp\n",
    "mlpModel= MLPClassifier(random_state=42, max_iter=2000)\n",
    "    #logisticreg\n",
    "lrModel= LogisticRegression(max_iter=120000)\n",
    "##pipelines\n",
    "mlpPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "        ('mlp', mlpModel)])\n",
    "rfPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "       # ('reduce_dims', PCA(n_components=4)),\n",
    "        ('rf', rfModel)])\n",
    "svcPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        #('reduce_dims', PCA(n_components=4)),\n",
    "        ('svc', svcModel)])\n",
    "knnPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        #('reduce_dims', PCA(n_components=4)),\n",
    "        ('knn', knnModel)])\n",
    "ldaPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "       # ('reduce_dims', PCA(n_components=4)),\n",
    "        ('lda', ldaModel)])\n",
    "lrPipe = Pipeline([\n",
    "        ('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "        ('lda', ldaModel)])\n",
    "cbPipe = Pipeline([\n",
    "        #('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "         ('rfe',RFECV(cbBestModel)),\n",
    "        ('cb', cbBestModel)])\n",
    "adaPipe = Pipeline([\n",
    "        #('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "     #   ('rfe',RFECV(cbBestModel)),\n",
    "        ('ada', adaBestModel)])\n",
    "\n",
    "\n",
    "\n",
    "adaBestModel\n",
    "##pipelines\n",
    "estimator=[('lda',ldaPipe),\n",
    "           ('cb',cbBestModel),\n",
    "           ('gbc',gbcModel),\n",
    "           ('rf',rfPipe),\n",
    "           ('xgb',xgbModel),\n",
    "           #('bagging',BaggingClassifier(n_estimators=10)),\n",
    "           ('svc',svcPipe),\n",
    "   #        ('knn',knnPipe),\n",
    "           ('mlp',mlpPipe),\n",
    "        #   ('lr',lrPipe),\n",
    "           ('ada',adaPipe)\n",
    "           ]\n",
    "\n",
    "\n",
    "clf = StackingClassifier(estimators=estimator,passthrough=True,final_estimator=lrPipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf=StratifiedKFold(shuffle=True, n_splits=5,random_state=seed)\n",
    "scores=cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train CV Mean Score: \"+str(round(scores.mean(),4))+\" +/- \"+str(round(scores.std()*2,4)))\n",
    "display(Markdown(\"\\n **Test Set Results**\\n\\n\"))\n",
    "testResult=classifier(clf,X_train,y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train .7751\n",
    "clf.score(X_test,y_test)\n",
    "stkCLF=clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classsifer ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)[:,1]\n",
    "ypred=y_pred\n",
    "X_valid=X_test.copy()\n",
    "X_valid.loc[:,\"Prediction\"]=y_pred\n",
    "#rounded prediction\n",
    "X_valid.loc[:,\"prediction\"]=np.round(y_pred)\n",
    "X_valid.loc[:,\"response\"]=y_test\n",
    "crt_filter=[]\n",
    "crt_df=[]\n",
    "tn=[None,None,None,None,None]\n",
    "fp=[None,None,None,None,None]\n",
    "tp=[None,None,None,None,None]\n",
    "fn=[None,None,None,None,None]\n",
    "for x in range(5):\n",
    "    crt_filter.append(X_valid['crt_score']==x)\n",
    "    crt_df.append(X_valid[crt_filter[x]])\n",
    "    tn[x], fp[x], fn[x], tp[x] = confusion_matrix(crt_df[x]['response'], crt_df[x]['prediction']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_roc_curve(crt_score='all'):\n",
    "    if crt_score == 'all':\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test,  ypred)\n",
    "        auc = metrics.roc_auc_score(y_test,  ypred)\n",
    "        auc=round(auc,4)\n",
    "        plt.plot(fpr,tpr,label=\"ALL auc=\"+str(auc),color='red')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "        #new\n",
    "        for crt_score in range(5):\n",
    "            crt_score\n",
    "            fpr, tpr, _ = metrics.roc_curve(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "            auc = metrics.roc_auc_score(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "            auc=round(auc,4)\n",
    "            plt.plot(fpr,tpr,label=\"CRT \"+str(crt_score)+\" auc=\"+str(auc))\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "            plt.title(\"ROC_AUC by CRT Score\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        #plt.title(\"ROC_AUC Full Validation Set\")\n",
    "        #plt.legend(loc=4)\n",
    "        #plt.show()\n",
    "    else: \n",
    "        fpr, tpr, _ = metrics.roc_curve(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "        auc = metrics.roc_auc_score(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "        auc=round(auc,4)\n",
    "        plt.plot(fpr,tpr,label=\"Stacked Model auc=\"+str(auc))\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "        plt.title(\"ROC_AUC for CRT Score:\"+str(crt_score))\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "show_roc_curve('all')    \n",
    "#show_roc_curve(0)\n",
    "#show_roc_curve(1)\n",
    "#show_roc_curve(2)\n",
    "#show_roc_curve(3)\n",
    "#show_roc_curve(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "#estimator=[('lda',LinearDiscriminantAnalysis()),('cb',cbBestModel),('gbc',gbcBestModel),('rf',rfBestModel),\n",
    "#                   ('knn',KNeighborsClassifier(5))                   ,('ada',AdaBoostClassifier(LogisticRegression()))\n",
    "#                   ,('xgb',xgBestModel),('gnb',GaussianNB()),('svc',SVC())]\n",
    "#Train CV Mean Score: 0.7894 +/- 0.1264\n",
    "#test .70\n",
    "\n",
    "#train Train CV Mean Score: 0.7836 +/- 0.1011\n",
    "#test .6920\n",
    "\n",
    "#Train CV Mean Score: 0.7804 +/- 0.0122\n",
    "#Test ROC_AUC:0.7046677215189874\n",
    "xgbModel=xgBestModel\n",
    "    #knn\n",
    "knnModel= knnBestModel\n",
    "    #svc\n",
    "svcModel=svcBestModel\n",
    "    #rf\n",
    "rfModel=rfBestModel\n",
    "    #gbc\n",
    "gbcModel=gbcBestModel\n",
    "    #lda\n",
    "ldaModel= LinearDiscriminantAnalysis()\n",
    "    #mlp\n",
    "mlpModel= MLPClassifier(random_state=42, max_iter=2000)\n",
    "    #logisticreg\n",
    "lrModel= LogisticRegression(max_iter=120000)\n",
    "##pipelines\n",
    "mlpPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "        ('mlp', mlpModel)])\n",
    "rfPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "       # ('reduce_dims', PCA(n_components=4)),\n",
    "        ('rf', rfModel)])\n",
    "svcPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        #('reduce_dims', PCA(n_components=4)),\n",
    "        ('svc', svcModel)])\n",
    "knnPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        #('reduce_dims', PCA(n_components=4)),\n",
    "        ('knn', knnModel)])\n",
    "ldaPipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "       # ('reduce_dims', PCA(n_components=4)),\n",
    "        ('lda', ldaModel)])\n",
    "lrPipe = Pipeline([\n",
    "        ('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "        ('lda', ldaModel)])\n",
    "cbPipe = Pipeline([\n",
    "        #('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "         ('rfe',RFECV(cbBestModel)),\n",
    "        ('cb', cbBestModel)])\n",
    "adaPipe = Pipeline([\n",
    "        #('scale', RobustScaler()),\n",
    "      #  ('reduce_dims', PCA(n_components=4)),\n",
    "     #   ('rfe',RFECV(cbBestModel)),\n",
    "        ('ada', adaBestModel)])\n",
    "\n",
    "\n",
    "adaBestModel\n",
    "##pipelines\n",
    "estimator=[('lda',ldaPipe),\n",
    "           ('cb',cbBestModel),\n",
    "           ('gbc',gbcModel),\n",
    "           ('rf',rfPipe),\n",
    "           ('xgb',xgbModel),\n",
    "           #('bagging',BaggingClassifier(n_estimators=10)),\n",
    "           ('svc',svcPipe),\n",
    "        #   ('knn',knnPipe),\n",
    "           ('mlp',mlpPipe),\n",
    "           ('lr',lrPipe),\n",
    "           ('ada',adaPipe)\n",
    "           ]\n",
    "\n",
    "#clf = StackingClassifier(estimators=estimator,final_estimator=lrPipe)\n",
    "\n",
    "clf = VotingClassifier(estimators=estimator, voting='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf=StratifiedKFold(shuffle=True, n_splits=5,random_state=seed)\n",
    "scores=cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train CV Mean Score: \"+str(round(scores.mean(),4))+\" +/- \"+str(round(scores.std()*2,4)))\n",
    "display(Markdown(\"\\n **Test Set Results**\\n\\n\"))\n",
    "testResult=classifier(clf,X_train,y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)[:,1]\n",
    "ypred=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc=metrics.roc_auc_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_base=[None]\n",
    "x=0\n",
    "for est in estimator:\n",
    "    print(\"Testing Classifer:\"+est[0])\n",
    "    _=est[1].fit(X_train, y_train)\n",
    "    scores=cross_val_score(_, X_train, y_train, scoring='roc_auc', cv=skf)\n",
    "    ypred_base.append(est[1].predict_proba(X_test)[:,1])\n",
    "    print(np.mean(scores))\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifer ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid=X_test.copy()\n",
    "X_valid.loc[:,\"Prediction\"]=y_pred\n",
    "#rounded prediction\n",
    "X_valid.loc[:,\"prediction\"]=np.round(y_pred)\n",
    "X_valid.loc[:,\"response\"]=y_test\n",
    "crt_filter=[]\n",
    "crt_df=[]\n",
    "tn=[None,None,None,None,None]\n",
    "fp=[None,None,None,None,None]\n",
    "tp=[None,None,None,None,None]\n",
    "fn=[None,None,None,None,None]\n",
    "for x in range(5):\n",
    "    crt_filter.append(X_valid['crt_score']==x)\n",
    "    crt_df.append(X_valid[crt_filter[x]])\n",
    "    tn[x], fp[x], fn[x], tp[x] = confusion_matrix(crt_df[x]['response'], crt_df[x]['prediction']).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves by CRT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_roc_curve(crt_score='all'):\n",
    "    if crt_score == 'all':\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test,  ypred)\n",
    "        auc = metrics.roc_auc_score(y_test,  ypred)\n",
    "        auc=round(auc,4)\n",
    "        plt.plot(fpr,tpr,label=\"ALL auc=\"+str(auc),color='red')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "        #new\n",
    "        for crt_score in range(5):\n",
    "            crt_score\n",
    "            fpr, tpr, _ = metrics.roc_curve(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "            auc = metrics.roc_auc_score(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "            auc=round(auc,4)\n",
    "            plt.plot(fpr,tpr,label=\"CRT \"+str(crt_score)+\" auc=\"+str(auc))\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "            plt.title(\"ROC_AUC by CRT Score\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        #plt.title(\"ROC_AUC Full Validation Set\")\n",
    "        #plt.legend(loc=4)\n",
    "        #plt.show()\n",
    "    else: \n",
    "        fpr, tpr, _ = metrics.roc_curve(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "        auc = metrics.roc_auc_score(crt_df[crt_score]['response'],  crt_df[crt_score]['prediction'])\n",
    "        auc=round(auc,4)\n",
    "        plt.plot(fpr,tpr,label=\"Stacked Model auc=\"+str(auc))\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "        plt.title(\"ROC_AUC for CRT Score:\"+str(crt_score))\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "show_roc_curve('all')    \n",
    "#show_roc_curve(0)\n",
    "#show_roc_curve(1)\n",
    "#show_roc_curve(2)\n",
    "#show_roc_curve(3)\n",
    "#show_roc_curve(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "r = [0,1,2,3,4]\n",
    "raw_data = {'greenBars': tp, 'orangeBars': tn,'blueBars': fp,'redBars':fn}\n",
    "df = pd.DataFrame(raw_data)\n",
    " \n",
    "# From raw value to percentage\n",
    "totals = [i+j+k+l for i,j,k,l in zip(df['greenBars'], df['orangeBars'], df['blueBars'],df['redBars'])]\n",
    "greenBars = [i / j * 100 for i,j in zip(df['greenBars'], totals)]\n",
    "orangeBars = [i / j * 100 for i,j in zip(df['orangeBars'], totals)]\n",
    "blueBars = [i / j * 100 for i,j in zip(df['blueBars'], totals)]\n",
    "redBars=[i / j * 100 for i,j in zip(df['redBars'], totals)]\n",
    "# plot\n",
    "barWidth = 0.85\n",
    "names = ('0','1','2','3','4')\n",
    "# Create green Bars\n",
    "plt.bar(r, greenBars, color='#b5ffb9', edgecolor='white', width=barWidth, label=\"TP\")\n",
    "# Create orange Bars\n",
    "plt.bar(r, orangeBars, bottom=greenBars, color='#f9bc86', edgecolor='white', width=barWidth, label=\"TN\")\n",
    "# Create blue Bars\n",
    "plt.bar(r, blueBars, bottom=[i+j for i,j in zip(greenBars, orangeBars)], color='#a3acff', edgecolor='white', width=barWidth, label=\"FP\")\n",
    "plt.bar(r, redBars, bottom=[i+j+k for i,j,k in zip(greenBars, orangeBars, blueBars)], color='#ffcccb', edgecolor='white', width=barWidth, label=\"FN\")\n",
    "# Custom x axis\n",
    "plt.xticks(r, names)\n",
    "plt.xlabel(\"CRT Score\")\n",
    "plt.ylabel(\"% of Samples\")\n",
    "# Add a legend\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    " \n",
    "# Show graphic\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize features using RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#determine optimal features to include in models that support feature importance\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "#from sklearn.svm import SVR\n",
    "#estimator = SVR(kernel=\"linear\")\n",
    "\n",
    "for est in estimator:\n",
    "\n",
    "    \n",
    "    try:\n",
    "        selector = RFECV(est[1])\n",
    "        selector = selector.fit(X, y)\n",
    "        print(est[0])\n",
    "        print(selector.support_)\n",
    "        print(selector.ranking_)\n",
    "        n_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=skf, n_jobs=-1, error_score='raise')\n",
    "        # report performance\n",
    "        print('ROC_AUC: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "    except:\n",
    "        print(est[0]+\" Exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "filename = 'votingclf.model'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "explainer = shap.KernelExplainer(clf.predict_proba, X, link=\"logit\")\n",
    "shapvalues = explainer.shap_values(X)\n",
    "#shap.summary_plot(explainer, X_test)\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# plot the SHAP values for the Setosa output of the first instance\n",
    "#shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shapvalues[0], X,plot_type='bar',max_display=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(shapvalues)\n",
    "#shap.plots.heatmap(shapvalues[1].toDF())\n",
    "shap.force_plot(explainer.expected_value[1], shapvalues[1], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = [i for i in X.columns if X[i].dtype in [np.int64, np.int64]]\n",
    "\n",
    "#shapDF=pd.DataFrame(shapvalues[1],columns=X.columns)\n",
    "#shap.plots.heatmap(shapvalues[0][:1000])\n",
    "shap.plots.scatter(shapvalues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shapvalues[1], X,show=False,max_display=X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('weight2',shapvalues[1], X,interaction_index='height2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(shapvalues,open(\"shap-scrt3.p\",\"wb\"))\n",
    "pickle.dump(explainer,open(\"exp-scrt3.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
